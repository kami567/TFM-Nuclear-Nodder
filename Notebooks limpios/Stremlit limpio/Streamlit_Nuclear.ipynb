{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Este es el codigo que se ejecuta en streamlit en formato Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Librerías básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import joblib\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Análisis y estadísticas\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Machine Learning y preprocesamiento\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Forecasting con Skforecast\n",
    "import skforecast\n",
    "from skforecast.datasets import fetch_dataset\n",
    "from skforecast.recursive import ForecasterRecursive\n",
    "from skforecast.direct import ForecasterDirect\n",
    "from skforecast.model_selection import TimeSeriesFold, grid_search_forecaster, backtesting_forecaster\n",
    "from skforecast.preprocessing import RollingFeatures\n",
    "from skforecast.utils import save_forecaster, load_forecaster\n",
    "\n",
    "# Interpretabilidad de modelos\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fecha_actual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "st.write(\"Fecha actual:\", fecha_actual)\n",
    "\n",
    "# Definir las rutas base\n",
    "ruta_sin_procesar = os.path.join(\"Datos\", \"Datos Sin Procesar\", fecha_actual)\n",
    "ruta_procesados = os.path.join(\"Datos\", \"Datos Procesados\", fecha_actual)\n",
    "\n",
    "# URL del archivo por defecto en GitHub\n",
    "url_tabla = \"https://raw.githubusercontent.com/kami567/TFM-Nuclear-Nodder/main/Table_8.1_Nuclear_Energy_Overview.xlsx\"\n",
    "# URL del scaler en GitHub (corrigiendo la ruta)\n",
    "url_scaler = \"https://raw.githubusercontent.com/kami567/TFM-Nuclear-Nodder/main/scalers.pkl\"\n",
    "# Definir las URLs de los modelos en GitHub\n",
    "url_sarimax_mw = \"https://raw.githubusercontent.com/kami567/TFM-Nuclear-Nodder/main/SARIMAX_Net_Summer_Capacity_MW.joblib\"\n",
    "url_sarimax_capacity = \"https://raw.githubusercontent.com/kami567/TFM-Nuclear-Nodder/main/SARIMAX_Capacity_Factor_Percent.joblib\"\n",
    "url_ridge = \"https://raw.githubusercontent.com/kami567/TFM-Nuclear-Nodder/main/modelo_ridge_NetGeneration.pkl\"\n",
    "\n",
    "# Directorios para guardar archivos localmente antes del commit en GitHub\n",
    "ruta_sin_procesar = \"archivos_sin_procesar\"\n",
    "ruta_procesados = \"archivos_procesados\"\n",
    "\n",
    "# Preguntar al usuario si quiere cargar desde GitHub o subir manualmente\n",
    "use_github = st.radio(\n",
    "    \"¿Cómo quieres cargar el archivo?\",\n",
    "    (\"Usar archivo por defecto de GitHub\", \"Subir un archivo propio\")\n",
    ")\n",
    "\n",
    "if use_github == \"Usar archivo por defecto de GitHub\":\n",
    "    # Cargar el archivo desde la URL\n",
    "    excel_data = pd.ExcelFile(url_tabla, engine=\"openpyxl\")\n",
    "    st.write(\"Archivo cargado desde GitHub\")\n",
    "else:\n",
    "    # Permitir la subida del archivo\n",
    "    uploaded_file = st.file_uploader(\"Sube un archivo Excel\", type=[\"xlsx\"])\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        excel_data = pd.ExcelFile(uploaded_file)\n",
    "        st.write(\"Archivo cargado desde el usuario\")\n",
    "    else:\n",
    "        st.warning(\"Por favor, sube un archivo para continuar.\")\n",
    "        st.stop()\n",
    "\n",
    "# Mostrar las hojas disponibles y permitir al usuario elegir\n",
    "sheet_name = st.selectbox(\"Selecciona la hoja de datos mensuales\", excel_data.sheet_names)\n",
    "\n",
    "# Cargar la hoja seleccionada\n",
    "df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "\n",
    "# Limpiar y procesar los datos\n",
    "monthly_data_clean = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "# Definir los encabezados de las columnas\n",
    "column_headers = [\n",
    "    \"Date\",\n",
    "    \"Total_Operable_Units\",\n",
    "    \"Net_Summer_Capacity_MW\",\n",
    "    \"Net_Generation_MWh\",\n",
    "    \"Share_of_Electricity_Percent\",\n",
    "    \"Capacity_Factor_Percent\"\n",
    "]\n",
    "\n",
    "# Extraer datos válidos\n",
    "monthly_data_extracted = monthly_data_clean.iloc[7:].reset_index(drop=True)\n",
    "monthly_data_extracted.columns = column_headers\n",
    "\n",
    "# Convertir la columna de fecha a formato datetime\n",
    "monthly_data_extracted['Date'] = pd.to_datetime(monthly_data_extracted['Date'], errors='coerce')\n",
    "monthly_data_extracted['Date'] = monthly_data_extracted['Date'].dt.to_period('M')\n",
    "\n",
    "# Convertir las columnas numéricas\n",
    "numeric_columns = column_headers[1:]\n",
    "for col in numeric_columns:\n",
    "    monthly_data_extracted[col] = pd.to_numeric(monthly_data_extracted[col], errors='coerce')\n",
    "\n",
    "monthly_data_extracted.set_index('Date', inplace=True)\n",
    "\n",
    "# Filtrar datos desde diciembre de 1994\n",
    "monthly_data_extracted = monthly_data_extracted.loc['1994-12-01':]\n",
    "\n",
    "# Calcular valores por unidad operativa\n",
    "monthly_data_per_unit = monthly_data_extracted.drop(columns=['Total_Operable_Units']).div(monthly_data_extracted['Total_Operable_Units'], axis=0)\n",
    "\n",
    "# Mostrar resultados\n",
    "st.write(f\"Mostrando datos de la hoja: {sheet_name} ya dividiendo por unidad operativa (Monthly Data Per Unit)\")\n",
    "st.write(monthly_data_per_unit.head(5))\n",
    "\n",
    "# Botón de descarga en Streamlit\n",
    "output = io.BytesIO()\n",
    "with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "    monthly_data_per_unit.to_excel(writer, sheet_name='Monthly Data Per Unit')\n",
    "    writer.close()\n",
    "\n",
    "st.download_button(\n",
    "    label=\"Descargar archivo procesado\",\n",
    "    data=output.getvalue(),\n",
    "    file_name=\"monthly_data_per_unit.xlsx\",\n",
    "    mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    ")\n",
    "\n",
    "# 🔹 Checkbox para permitir guardar en GitHub\n",
    "guardar_en_github = st.checkbox(\"Guardar archivos en GitHub\")\n",
    "\n",
    "if guardar_en_github:\n",
    "    # Asegurar que las subcarpetas existan\n",
    "    os.makedirs(ruta_sin_procesar, exist_ok=True)\n",
    "    os.makedirs(ruta_procesados, exist_ok=True)\n",
    "\n",
    "    if use_github == \"Subir un archivo propio\" and uploaded_file is not None:\n",
    "        # Guardar el archivo sin procesar en la carpeta correspondiente\n",
    "        ruta_archivo_original = os.path.join(ruta_sin_procesar, uploaded_file.name)\n",
    "        with open(ruta_archivo_original, \"wb\") as f:\n",
    "            f.write(uploaded_file.getbuffer())\n",
    "\n",
    "    # Guardar el archivo procesado en la carpeta de datos procesados\n",
    "    ruta_archivo_procesado = os.path.join(ruta_procesados, \"monthly_data_per_unit.xlsx\")\n",
    "    with pd.ExcelWriter(ruta_archivo_procesado, engine='xlsxwriter') as writer:\n",
    "        monthly_data_per_unit.to_excel(writer, sheet_name='Monthly Data Per Unit')\n",
    "        writer.close()\n",
    "\n",
    "    st.success(f\"Archivos guardados en:\\n- {ruta_sin_procesar}\\n- {ruta_procesados}\")\n",
    "\n",
    "    # COMMIT AUTOMÁTICO EN GIT\n",
    "    os.system(f\"git add {ruta_sin_procesar} {ruta_procesados}\")\n",
    "    os.system(f'git commit -m \"Añadidos archivos sin procesar y procesados del {fecha_actual}\"')\n",
    "    os.system(\"git push origin main\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Descargar el archivo desde GitHub\n",
    "response = requests.get(url_scaler)\n",
    "if response.status_code == 200:\n",
    "    scaler = joblib.load(io.BytesIO(response.content))\n",
    "    print(\"Scaler cargado correctamente desde GitHub.\")\n",
    "else:\n",
    "    print(\"Error al descargar el scaler:\", response.status_code)\n",
    "\n",
    "\n",
    "\n",
    "# Función para cargar modelos desde GitHub\n",
    "def cargar_modelo(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return joblib.load(io.BytesIO(response.content))\n",
    "    else:\n",
    "        print(f\"Error al descargar {url}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Cargar los modelos\n",
    "SARIMAX_Net_Summer_Capacity_MW = cargar_modelo(url_sarimax_mw)\n",
    "SARIMAX_Capacity_Factor_Percent = cargar_modelo(url_sarimax_capacity)\n",
    "modelo_ridge_NetGeneration = cargar_modelo(url_ridge)\n",
    "scalers = cargar_modelo(url_scaler)  # Cargamos los scalers\n",
    "\n",
    "# Verificar que monthly_data_per_unit exista antes de crear X e y\n",
    "if 'monthly_data_per_unit' in locals():\n",
    "\n",
    "    # Definir X (variables predictoras) e Y (variable objetivo)\n",
    "    X = monthly_data_per_unit.drop(columns=['Net_Generation_MWh', 'Share_of_Electricity_Percent'])\n",
    "    y = monthly_data_per_unit['Net_Generation_MWh']\n",
    "    \n",
    "    # Obtener el scaler correcto\n",
    "    scaler_X = scalers.get(\"Net_Summer_Capacity_MW\", None)\n",
    "\n",
    "    if scaler_X is None:\n",
    "        st.error(\"Error: No se encontró el scaler para 'Net_Summer_Capacity_MW'\")\n",
    "    else:\n",
    "        # Verificar columnas esperadas vs actuales\n",
    "        st.write(\"Las variables exógenas son:\", X.columns.tolist())\n",
    "\n",
    "        # Asegurar que las columnas coinciden\n",
    "        for col in scaler_X.feature_names_in_:\n",
    "            if col not in X.columns:\n",
    "                X[col] = 0  # O usa np.nan para marcar los valores faltantes\n",
    "\n",
    "        # Reordenar las columnas\n",
    "        X = X[scaler_X.feature_names_in_]\n",
    "\n",
    "        # Aplicar la transformación\n",
    "        X_scaled = scaler_X.transform(X)\n",
    "        \n",
    "else:\n",
    "    st.error(\"Error: `monthly_data_per_unit` no está definido. Asegúrate de procesar correctamente los datos.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Selección de años a predecir\n",
    "años_SARIMAX = st.number_input(\"¿Cuántos años quieres predecir?\", min_value=1, max_value=15, value=10, step=1, key = \"Años SARIMAX\")\n",
    "\n",
    "# Interfaz en Streamlit\n",
    "st.title(\"Predicción con SARIMAX de variables exógenas\")\n",
    "\n",
    "# Checkbox para decidir si predecir con SARIMAX\n",
    "usar_sarimax = st.checkbox(\"¿Quieres predecir variables exógenas con SARIMAX?\")\n",
    "\n",
    "if usar_sarimax:\n",
    "\n",
    "\n",
    "    # Convertir años a pasos (steps) de predicción\n",
    "    steps = años_SARIMAX * 12  # 12 meses por año\n",
    "\n",
    "    # Realizar predicciones con SARIMAX\n",
    "    pred_sarimax_Net_Summer = SARIMAX_Net_Summer_Capacity_MW.forecast(steps=steps)\n",
    "    pred_sarimax_Capacity = SARIMAX_Capacity_Factor_Percent.forecast(steps=steps)\n",
    "\n",
    "\n",
    "    # Inversa la transformación\n",
    "    Net_summer_capacity_predict = scalers['Net_Summer_Capacity_MW'].inverse_transform(pred_sarimax_Net_Summer.values.reshape(-1, 1))\n",
    "    Capacity_factor_predict = scalers['Capacity_Factor_Percent'].inverse_transform(pred_sarimax_Capacity.values.reshape(-1, 1))\n",
    "    # Generar fechas futuras\n",
    "    future_dates = pd.date_range(start=pd.Timestamp.today().replace(day=1), periods=len(Net_summer_capacity_predict), freq=\"MS\").to_period('M')\n",
    "\n",
    "    # Convertir a DataFrame con índice de fechas\n",
    "    df_predictions = pd.DataFrame({\n",
    "        \"Fecha\": future_dates,\n",
    "        \"Predicción Net Summer Capacity MW\": Net_summer_capacity_predict.flatten(),  # Asegurar 1D\n",
    "        \"Predicción Capacity Factor Percent\": Capacity_factor_predict.flatten()  # Asegurar 1D\n",
    "    })\n",
    "\n",
    "    #Definimos el historico de datos\n",
    "    historico_x = monthly_data_per_unit[[\"Net_Summer_Capacity_MW\", \"Capacity_Factor_Percent\"]].copy()\n",
    "\n",
    "    # 🔹 Asegurar que el índice del histórico es DatetimeIndex\n",
    "    if not isinstance(historico_x.index, pd.DatetimeIndex):\n",
    "        historico_x.index = historico_x.index.to_timestamp()\n",
    "\n",
    "    # Definir la columna Fecha como índice\n",
    "    df_predictions.set_index(\"Fecha\", inplace=True)\n",
    "    df_predictions.index =df_predictions.index.to_timestamp()\n",
    "\n",
    "    # 🔍 Depuración antes de mostrar\n",
    "    st.write(\"Primeras filas del DataFrame de predicciones:\")\n",
    "    st.dataframe(df_predictions.head())\n",
    "\n",
    "\n",
    "\n",
    "    # 🔹 Crear subgráficos con dos filas\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=[\n",
    "        \"Predicción Net Summer Capacity MW\",\n",
    "        \"Predicción Capacity Factor Percent\"\n",
    "    ])\n",
    "\n",
    "    # 🔹 Primer gráfico (Net Summer Capacity MW)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_predictions.index, \n",
    "        y=df_predictions[\"Predicción Net Summer Capacity MW\"], \n",
    "        mode='lines', \n",
    "        name=\"Predicción\", \n",
    "        line=dict(color=\"gold\")\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=historico_x.index, \n",
    "        y=historico_x[\"Net_Summer_Capacity_MW\"], \n",
    "        mode='lines', \n",
    "        name=\"Histórico\", \n",
    "        line=dict(color=\"green\")\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # 🔹 Segundo gráfico (Capacity Factor Percent)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_predictions.index, \n",
    "        y=df_predictions[\"Predicción Capacity Factor Percent\"], \n",
    "        mode='lines', \n",
    "        name=\"Predicción\", \n",
    "        line=dict(color=\"gold\")\n",
    "    ), row=2, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=historico_x.index, \n",
    "        y=historico_x[\"Capacity_Factor_Percent\"], \n",
    "        mode='lines', \n",
    "        name=\"Histórico\", \n",
    "        line=dict(color=\"darkgreen\")\n",
    "    ), row=2, col=1)\n",
    "\n",
    "    # 🔹 Configurar el diseño del gráfico\n",
    "    fig.update_layout(\n",
    "        height=800, width=1000,  # Tamaño personalizado\n",
    "        title_text=\"Predicciones de Energía Nuclear\",\n",
    "        showlegend=True,  # Mostrar leyenda en ambos gráficos\n",
    "        legend=dict(\n",
    "            x=0,  # Posicionar a la izquierda\n",
    "            y=0.5,  # Posicionar arriba\n",
    "            bgcolor=\"rgba(0,0,0,0)\"  # Fondo transparente\n",
    "        ),\n",
    "        template=\"plotly_dark\"  # Modo oscuro\n",
    "    )\n",
    "    \n",
    "    # 🔹 Mostrar el gráfico en Streamlit\n",
    "    st.plotly_chart(fig)\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Botón para descargar resultados como CSV\n",
    "    csv = df_predictions.to_csv(index=False).encode('utf-8')\n",
    "    st.download_button(\n",
    "        label=\"Descargar predicciones como CSV\",\n",
    "        data=csv,\n",
    "        file_name=\"predicciones_sarimax.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "else:\n",
    "    st.warning(\"No se realiza predicción con SARIMAX.\")\n",
    "# Interfaz en Streamlit\n",
    "st.title(\"Predicción con Ridge\")\n",
    "# Selección de años a predecir\n",
    "años_Ridge = st.number_input(\"¿Cuántos años quieres predecir?\", min_value=1, max_value=15, value=10, step=1, key = \"Años Ridge\")\n",
    "\n",
    "# Checkbox para decidir si predecir con Ridge\n",
    "usar_ridge = st.checkbox(\"¿Quieres predecir Net Generation MWh con Ridge?\")\n",
    "\n",
    "if usar_ridge:\n",
    "    # Definir Y_escalado correctamente\n",
    "    Y_escalado = y  # Asegurar que y ha sido escalado previamente\n",
    "    \n",
    "    # Convertir años a pasos (steps) de predicción\n",
    "    steps = años_Ridge * 12  # 12 meses por año\n",
    "\n",
    "    # Mostrar configuración del modelo\n",
    "    st.write(\"### 🔍 Configuración del modelo Ridge\")\n",
    "    st.write(f\"Window size original del modelo: {modelo_ridge_NetGeneration.window_size}\")\n",
    "\n",
    "    # Ajustar window_size si hay menos datos de los esperados\n",
    "    window_size = modelo_ridge_NetGeneration.window_size\n",
    "    if len(Y_escalado) < window_size:\n",
    "        window_size = len(Y_escalado)  # Ajustamos al máximo posible\n",
    "        st.warning(f\"⚠️ Window size ajustado a {window_size} porque no hay suficientes datos.\")\n",
    "\n",
    "    # Obtener la última ventana de datos\n",
    "    last_window = Y_escalado[-modelo_ridge_NetGeneration.window_size:]\n",
    "    last_window.index = last_window.index.to_timestamp()\n",
    "    last_window = last_window.asfreq('MS')\n",
    "\n",
    "    #Usamos el modelo para predecir los años seleccionados\n",
    "    pred_ridge_NetGeneration = modelo_ridge_NetGeneration.predict(steps=steps, last_window=last_window)\n",
    "\n",
    "    #Cambiamos de serie a DF\n",
    "    df_pred = pred_ridge_NetGeneration.rename_axis(\"Fecha\").rename(\"Predicción Net Generation MWh\").to_frame()\n",
    "\n",
    "    # Mostrar predicciones en tabla\n",
    "    st.write(\"###Predicciones Net Generation MWh\")\n",
    "    st.dataframe(df_pred.style.format(\"{:.2f}\"))\n",
    "    #st.dataframe(df_predictions)\n",
    "\n",
    "    # Graficar la serie original y la predicción suavizadas\n",
    "    st.write(\"###Comparación entre Datos Originales y Predicción\")\n",
    "\n",
    "    # Asegurar que Y_escalado y pred_ridge_NetGeneration sean Series numéricas\n",
    "    Y_suavizado = Y_escalado.rolling(window=4, min_periods=1).mean()\n",
    "    pred_suavizado = pred_ridge_NetGeneration.rolling(window=4, min_periods=1).mean()\n",
    "\n",
    "\n",
    "    # ✅ Eliminar valores NaN\n",
    "    Y_suavizado = Y_suavizado.dropna()\n",
    "    pred_suavizado = pred_suavizado.dropna()\n",
    "\n",
    "    # ✅ Asegurar que el índice es un DatetimeIndex\n",
    "    if not isinstance(Y_suavizado.index, pd.DatetimeIndex):\n",
    "        Y_suavizado.index = Y_suavizado.index.to_timestamp()\n",
    "\n",
    "    if not isinstance(pred_suavizado.index, pd.DatetimeIndex):\n",
    "        pred_suavizado.index = pred_suavizado.index.to_timestamp()\n",
    "\n",
    "    # ✅ Asegurar que los datos sean numéricos\n",
    "    Y_suavizado = Y_suavizado.astype(float)\n",
    "    pred_suavizado = pred_suavizado.astype(float)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # 🔹 Crear la figura en Plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # 🔹 Agregar la serie original\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=Y_suavizado.index, \n",
    "        y=Y_suavizado, \n",
    "        mode='lines', \n",
    "        name=\"Datos Originales\", \n",
    "        line=dict(color=\"limegreen\")\n",
    "    ))\n",
    "\n",
    "    # 🔹 Agregar la predicción\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pred_suavizado.index, \n",
    "        y=pred_suavizado, \n",
    "        mode='lines', \n",
    "        name=\"Predicción Ridge\", \n",
    "        line=dict(color=\"gold\")\n",
    "    ))\n",
    "\n",
    "    # 🔹 Configurar el diseño del gráfico\n",
    "    fig.update_layout(\n",
    "        title=\"Comparación entre Datos Originales y Predicción Ridge (Suavizado)\",\n",
    "        xaxis_title=\"Fecha\",\n",
    "        yaxis_title=\"MWh\",\n",
    "        legend=dict(\n",
    "            x=0,  # Posición en la izquierda\n",
    "            y=1.1,  # Posición arriba del gráfico\n",
    "            font=dict(color=\"white\"),  # Letras blancas en la leyenda\n",
    "            bgcolor=\"rgba(0,0,0,0)\"  # Fondo transparente para la leyenda\n",
    "        ),\n",
    "        template=\"plotly_dark\",  # Fondo negro con letras blancas\n",
    "        width=800, height=500  # Ajuste de tamaño para mejor visualización\n",
    "        )\n",
    "\n",
    "    # 🔹 Mostrar la gráfica en Streamlit\n",
    "    st.plotly_chart(fig)\n",
    "\n",
    "\n",
    "    # Botón para descargar predicciones en CSV\n",
    "    csv = df_predictions.to_csv(index=False).encode('utf-8')\n",
    "    st.download_button(\n",
    "        label=\"📥 Descargar predicciones como CSV\",\n",
    "        data=csv,\n",
    "        file_name=\"predicciones_ridge.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "\n",
    "    # Preguntar si el usuario quiere guardar los datos en GitHub\n",
    "    guardar_github = st.checkbox(\"¿Quieres guardar las predicciones en GitHub?\")\n",
    "    if guardar_github:\n",
    "        import os\n",
    "\n",
    "        # Definir ruta de guardado\n",
    "        ruta_base = \"Datos/Predicciones Ridge\"\n",
    "        ruta_subcarpeta = os.path.join(ruta_base, pd.Timestamp.today().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        # Asegurar que la subcarpeta existe\n",
    "        os.makedirs(ruta_subcarpeta, exist_ok=True)\n",
    "\n",
    "        # Guardar el archivo en la carpeta\n",
    "        ruta_archivo = os.path.join(ruta_subcarpeta, \"predicciones_ridge.csv\")\n",
    "        df_predictions.to_csv(ruta_archivo, index=False)\n",
    "\n",
    "        # Subir a GitHub con Git\n",
    "        os.system(f\"git add {ruta_archivo}\")\n",
    "        os.system(f'git commit -m \"Añadidas predicciones Ridge del {pd.Timestamp.today().strftime(\"%Y-%m-%d\")}\"')\n",
    "        os.system(\"git push origin main\")\n",
    "\n",
    "        st.success(f\"📤 Predicciones guardadas en GitHub: {ruta_archivo}\")\n",
    "\n",
    "else:\n",
    "    st.warning(\"No se realizó predicción con Ridge.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
